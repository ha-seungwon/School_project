{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Suppress LightGBM info messages\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Read the training and test data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('testset.csv')\n",
    "\n",
    "train_df=train_df.fillna(0)\n",
    "test_df=test_df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define categorical columns for label encoding\n",
    "categorical_columns = [\"Gender\", \"V17\", \"V19\"]\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    train_df[column] = label_encoder.fit_transform(train_df[column])\n",
    "    test_df[column] = label_encoder.transform(test_df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = train_df.drop(columns=['class'])\n",
    "y = train_df['class']\n",
    "y = y.replace({'N': 0, 'Y': 1})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "cat_model = CatBoostClassifier(random_state=42)\n",
    "xgb_cls = XGBClassifier(random_state=42)\n",
    "gra_model = GradientBoostingClassifier(random_state=42)\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "lightgbm = LGBMClassifier(random_state=42)\n",
    "\n",
    "models_list = [rf_classifier,cat_model,xgb_cls, gra_model]\n",
    "\n",
    "result = []\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "for model in models_list:\n",
    "    print(model)\n",
    "    scores = cross_val_score(model, X_train[['V5',\n",
    "   'V6',\n",
    "   'V7',\n",
    "   'V8',\n",
    "   'V9',\n",
    "   'V10',\n",
    "   'V11',\n",
    "   'V12',\n",
    "   'V13',\n",
    "   'V14',\n",
    "   'V15',\n",
    "   'V16',\n",
    "   'V19',\n",
    "   'V22']], y_train, cv=5)\n",
    "    result.append(scores.mean())\n",
    "\n",
    "# Print model results\n",
    "print(\"Model results:\")\n",
    "print(result)\n",
    "#[0.8903225806451612, 0.8833333333333334, 0.875268817204301, 0.8887096774193548]\n",
    "#[0.8903225806451612, 0.8833333333333334, 0.875268817204301, 0.8890681003584231] standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "celect_col=['V5',\n",
    "   'V6',\n",
    "   'V7',\n",
    "   'V8',\n",
    "   'V9',\n",
    "   'V10',\n",
    "   'V11',\n",
    "   'V12',\n",
    "   'V13',\n",
    "   'V14',\n",
    "   'V15',\n",
    "   'V16',\n",
    "   'V19',\n",
    "   'V22']\n",
    "def objectiveCAT(trial: Trial, x_tr, y_tr, x_val, y_val):\n",
    "    param = {\n",
    "        'iterations' : trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth' : trial.suggest_int('depth', 3, 8),\n",
    "        'learning_rate' : trial.suggest_float('learning_rate', 0.001, 0.1),\n",
    "        'random_state' : 42}\n",
    "        #'class_weights': weight}\n",
    "    # 학습 모델 생성\n",
    "    model = CatBoostClassifier(**param)\n",
    "    cat_model = model.fit(x_tr, y_tr)\n",
    "    \n",
    "    # 모델 성능 확인\n",
    "    pred = cat_model.predict(x_val)\n",
    "    \n",
    "    score = f1_score(y_val, pred, average=\"binary\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "# 하이퍼 파라미터 튜닝\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=0))\n",
    "study.optimize(lambda trial : objectiveCAT(trial, X_train[celect_col], y_train, X_test[celect_col], y_test), n_trials = 30)\n",
    "\n",
    "print('Best trial : score {}, \\nparams {}'.format(study.best_trial.value, study.best_trial.params))\n",
    "\n",
    "cat_model=CatBoostClassifier(**study.best_trial.params,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define the individual classifiers\n",
    "xgb_cls = XGBClassifier(random_state=42)\n",
    "lightgbm = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Create a list of tuples, where each tuple contains a name for the classifier\n",
    "# and the classifier instance\n",
    "classifiers = [\n",
    "    ('XGBoost', xgb_cls),\n",
    "    ('CatBoost', cat_model),\n",
    "    ('LightGBM', lightgbm)\n",
    "]\n",
    "\n",
    "# Create a VotingClassifier using a soft voting strategy\n",
    "voting_model = VotingClassifier(estimators=classifiers, voting='soft')\n",
    "\n",
    "# Fit the VotingClassifier to the training data\n",
    "voting_model.fit(X_train[['V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V19', 'V22']], y_train)\n",
    "\n",
    "# Perform cross-validation for the voting model\n",
    "scores = cross_val_score(voting_model, X_train[['V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V19', 'V22']], y_train, cv=5)\n",
    "\n",
    "cat_scores = cross_val_score(cat_model, X_train[['V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V19', 'V22']], y_train, cv=5)\n",
    "print(\"Voting Model Results:\")\n",
    "print(scores.mean())\n",
    "\n",
    "print(\"cat Model Results:\")\n",
    "print(cat_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=cat_model\n",
    "# Train the LightGBM model on the entire dataset\n",
    "model.fit(X[['V5',\n",
    "   'V6',\n",
    "   'V7',\n",
    "   'V8',\n",
    "   'V9',\n",
    "   'V10',\n",
    "   'V11',\n",
    "   'V12',\n",
    "   'V13',\n",
    "   'V14',\n",
    "   'V15',\n",
    "   'V16',\n",
    "   'V19',\n",
    "   'V22']], y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "pred = model.predict(test_df[['V5',\n",
    "   'V6',\n",
    "   'V7',\n",
    "   'V8',\n",
    "   'V9',\n",
    "   'V10',\n",
    "   'V11',\n",
    "   'V12',\n",
    "   'V13',\n",
    "   'V14',\n",
    "   'V15',\n",
    "   'V16',\n",
    "   'V19',\n",
    "   'V22']])\n",
    "\n",
    "# Convert NumPy array to DataFrame and save as a CSV file\n",
    "result = np.where(pred == 0, 'N', 'Y')\n",
    "output_csv = pd.DataFrame(result, columns=['class'])\n",
    "output_csv.to_csv(\"32184801_하승원.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your ranking: 1 , acc: 0.896851975887475 , total upload: 1\n",
    "#Your ranking: 1 , acc: 0.912257200267917 , total upload: 2\n",
    "#Your ranking: 1 , acc: 0.912257200267917 , total upload: 3\n",
    "#Your ranking: 1 , acc: 0.916275954454119 , total upload: 4\n",
    "#Your ranking: 2 , acc: 0.916275954454119 , total upload: 5\n",
    "#Your ranking: 3 , acc: 0.916275954454119 , total upload: 8\n",
    "#Your ranking: 3 , acc: 0.916275954454119 , total upload: 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 별 의미없는거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "#scaler = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "\n",
    "# # Select the columns to be scaled\n",
    "# columns_to_scale = [\"V8\", \"V13\", \"V20\", \"V21\", \"V22\"]\n",
    "# train_df[columns_to_scale] = np.log1p(train_df[columns_to_scale])\n",
    "# test_df[columns_to_scale] = np.log1p(test_df[columns_to_scale])\n",
    "# # Initialize the StandardScaler\n",
    "\n",
    "# #scaler = RobustScaler()\n",
    "# scaler = QuantileTransformer(output_distribution='normal')\n",
    "# # Fit and transform the selected columns in the training data\n",
    "# train_df[columns_to_scale] = scaler.fit_transform(train_df[columns_to_scale])\n",
    "\n",
    "# # Transform the corresponding columns in the test data\n",
    "# test_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate,cross_val_score\n",
    "\n",
    "model=cat_model\n",
    "\n",
    "rfe_result=[]\n",
    "rfe_feature=[]\n",
    "for i in range(1,len(X.columns)):\n",
    "    rfe= RFE(model, n_features_to_select=i)\n",
    "    fit = rfe.fit(X, y)\n",
    "    fs = X.columns[fit.support_].tolist()   # selected features\n",
    "    rfe_feature.append(fs)\n",
    "    scores = cross_val_score(model, X[fs], y, cv=5)\n",
    "    rfe_result.append(scores.mean())\n",
    "\n",
    "print(rfe_result)\n",
    "print(rfe_feature)\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "# from sklearn.feature_selection import RFE\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import cross_validate,cross_val_score\n",
    "\n",
    "# #forward selection\n",
    "\n",
    "# sfs1 = SFS(model, k_features=len(X.columns),          # number of features\n",
    "#             verbose=2,scoring='accuracy',cv=5)\n",
    "# sfs1 = sfs1.fit(X, y)\n",
    "# print(sfs1.subsets_)             # selection process\n",
    "# print(sfs1.k_feature_idx_)       # selected feature index  \n",
    "# print(sfs1.k_feature_names_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
