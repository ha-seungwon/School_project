{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#튜닝 전\n",
    "#[0.9004298  0.90537634 0.89677419 0.88100358 0.87670251]\n",
    "#0.8920572860502614\n",
    "\n",
    "#각 폴드에서의 정확도: [0.9004298  0.90537634 0.89677419 0.88100358 0.87670251]\n",
    "#평균 정확도: 0.8920572860502614\n",
    "#각 폴드에서의 정확도: [0.90401146 0.90537634 0.89032258 0.87598566 0.87455197]\n",
    "#평균 정확도: 0.8900496040915673\n",
    "#각 폴드에서의 정확도: [0.9004298  0.90967742 0.89175627 0.87885305 0.87956989]\n",
    "#평균 정확도: 0.8920572860502614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "random_state_num=42\n",
    "\n",
    "# Suppress LightGBM info messages\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Read the training and test data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('testset.csv')\n",
    "\n",
    "# Check for missing values in the training data\n",
    "missing_train = train_df.isnull().sum()\n",
    "print(\"Missing values in training data:\\n\", missing_train[missing_train > 0])\n",
    "\n",
    "# Check for missing values in the test data\n",
    "missing_test = test_df.isnull().sum()\n",
    "print(\"Missing values in test data:\\n\", missing_test[missing_test > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"Gender\", \"V17\", \"V19\"]\n",
    "\n",
    "#Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    train_df[column] = label_encoder.fit_transform(train_df[column])\n",
    "    test_df[column] = label_encoder.transform(test_df[column])\n",
    "    \n",
    "    \n",
    "# Split the data into features (X) and target (y)\n",
    "X = train_df.drop(columns=['class'])\n",
    "y = train_df['class']\n",
    "y = y.replace({'N': 0, 'Y': 1})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_model=CatBoostClassifier(depth=6, random_state=random_state_num)\n",
    "test_model.fit(X_train[X.columns], y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = test_model.predict(X_test[X.columns])\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "# from sklearn.feature_selection import RFE\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import cross_validate,cross_val_score\n",
    "\n",
    "# #forward selection\n",
    "# cat_model = CatBoostClassifier(depth=6,random_state=random_state_num)\n",
    "\n",
    "# sfs1 = SFS(cat_model, k_features=len(X.columns),          # number of features\n",
    "#             verbose=2,scoring='accuracy',cv=2)\n",
    "# sfs1 = sfs1.fit(X, y)\n",
    "# print(sfs1.subsets_)             # selection process\n",
    "# print(sfs1.k_feature_idx_)       # selected feature index\n",
    "# print(sfs1.k_feature_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# depth\n",
    "\n",
    "Tree depthTree depth\n",
    "In most cases, the optimal depth ranges from 4 to 10. Values in the range from 6 to 10 are recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_col=['V5',\n",
    "   'V6',\n",
    "   'V7',\n",
    "   'V8',\n",
    "   'V9',\n",
    "   'V10',\n",
    "   'V11',\n",
    "   'V12',\n",
    "   'V13',\n",
    "   'V14',\n",
    "   'V15',\n",
    "   'V16',\n",
    "   'V19',\n",
    "   'V22']\n",
    "\n",
    "X=X[select_col]\n",
    "test_df=test_df[select_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = CatBoostClassifier(depth=6,random_state=random_state_num)\n",
    "model=cat_model\n",
    "# Train the LightGBM model on the entire dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "pred = model.predict(test_df)\n",
    "\n",
    "print(pred)\n",
    "# Convert NumPy array to DataFrame and save as a CSV file\n",
    "result = np.where(pred == 0, 'N', 'Y')\n",
    "output_csv = pd.DataFrame(result, columns=['class'])\n",
    "output_csv.to_csv(\"32184801_하승원.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_csv=pd.read_csv('32184801_하승원_top.csv')\n",
    "output_csv=pd.read_csv('32184801_하승원.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "# Create a TabNetClassifier model\n",
    "model = TabNetClassifier(seed=42,optimizer_params=dict(lr=1e-2),\n",
    ")\n",
    "\n",
    "# Train the TabNet model on the training data\n",
    "model.fit(X_train[X.columns].values, y_train,\n",
    "        max_epochs=200,\n",
    "        batch_size=32,\n",
    ")\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = model.predict(X_test[X.columns].values)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"TabNet Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Example: Add classifiers from external libraries\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "model1 = RandomForestClassifier(random_state=random_state_num)\n",
    "model2 = XGBClassifier(random_state=random_state_num)\n",
    "model3 = GradientBoostingClassifier(random_state=random_state_num)\n",
    "model4 = LogisticRegression(random_state=random_state_num)\n",
    "model5 = LGBMClassifier(random_state=random_state_num)\n",
    "model6 = SVC(kernel='linear', random_state=random_state_num)\n",
    "model7 = DecisionTreeClassifier(random_state=random_state_num)\n",
    "\n",
    "# Example: Add more classifiers\n",
    "model8 = CatBoostClassifier()\n",
    "model9 = GaussianNB()\n",
    "model10 = MLPClassifier(random_state=random_state_num)\n",
    "model11 = AdaBoostClassifier(random_state=random_state_num)\n",
    "model12 = BaggingClassifier(random_state=random_state_num)\n",
    "model13 = ExtraTreesClassifier(random_state=random_state_num)\n",
    "model14 = KNeighborsClassifier()\n",
    "model15 = VotingClassifier(estimators=[('rf', model1), ('xgb', model2), ('svc', model6)], voting='hard')\n",
    "model16 = StackingClassifier(estimators=[('rf', model1), ('xgb', model2)], final_estimator=model4)\n",
    "model17 = HistGradientBoostingClassifier(random_state=random_state_num)\n",
    "\n",
    "models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10,\n",
    "          model11, model12, model13, model14, model15, model16, model17]\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    result = accuracy_score(y_test, pred)\n",
    "    results.append(result)\n",
    "    print(f\"Model {index + 1}: Accuracy = {result:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "for col in X.columns:\n",
    "    print(col)\n",
    "    print(X[col].unique())\n",
    "\n",
    "\n",
    "scaled_data = X[['V20','V21','V22']]\n",
    "test_scaled_data=test_df[['V20','V21','V22']]\n",
    "\n",
    "\n",
    "# StandardScaler를 사용하여 표준화\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(scaled_data)\n",
    "test_standardized = scaler.fit_transform(test_scaled_data)\n",
    "\n",
    "print(\"StandardScaler output\")\n",
    "print(X_standardized,test_standardized)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_standardized = scaler.fit_transform(scaled_data)\n",
    "test_standardized = scaler.fit_transform(test_scaled_data)\n",
    "print(\"MinMaxScaler output\")\n",
    "print(X_standardized,test_standardized)\n",
    "\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_standardized = scaler.fit_transform(scaled_data)\n",
    "test_standardized = scaler.fit_transform(test_scaled_data)\n",
    "print(\"RobustScaler output\")\n",
    "print(X_standardized,test_standardized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Selection -> 여러 모델링 실험결과 종류가 다른 모델 여러개를 앙상블 하는 것이 좋다 판단함\n",
    "class CFG:\n",
    "    SEED = random_state_num\n",
    "    \n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, VotingClassifier,RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=CFG.SEED)),\n",
    "    ('rc', RidgeClassifier(random_state=CFG.SEED)),\n",
    "    ('xgb', XGBClassifier(random_state=CFG.SEED)),\n",
    "    ('lgb', LGBMClassifier(random_state=CFG.SEED)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=CFG.SEED)),\n",
    "    ('rcc', RidgeClassifierCV()),\n",
    "    ('rf', RandomForestClassifier(random_state=CFG.SEED)),\n",
    "    ('cat',CatBoostClassifier(depth=6,random_state=CFG.SEED))\n",
    "    \n",
    "]\n",
    "best_model  = VotingClassifier(models, voting='hard', weights=[1,1,2,1,1,1,1,2])\n",
    "best_model.fit(X_train[X.columns], y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = best_model.predict(X_test[X.columns])\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_model=CatBoostClassifier(depth=6)\n",
    "test_model.fit(X_train[X.columns], y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = test_model.predict(X_test[X.columns])\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# results=[]\n",
    "# for i in learning_rate:\n",
    "#     cat_model = CatBoostClassifier(eval_metric=i,depth=6,random_state=random_state_num)\n",
    "\n",
    "#     # 5-폴드 교차 검증을 사용하여 모델의 정확도를 계산\n",
    "#     scores = cross_val_score(cat_model, X, y, cv=5, scoring='accuracy')\n",
    "#     results.append([scores,scores.mean()])\n",
    "\n",
    "# for i in range(len(results)):\n",
    "#     # 각 폴드에서의 정확도 및 평균 정확도 출력\n",
    "#     print(\"각 폴드에서의 정확도:\", results[i][0])\n",
    "#     print(\"평균 정확도:\", results[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate,cross_val_score\n",
    "\n",
    "\n",
    "for i in range(1,len(X.columns)):\n",
    "    print(i)\n",
    "    rfe= RFE(model, n_features_to_select=i)\n",
    "    fit = rfe.fit(X, y)\n",
    "    print(\"Num Features: %d\" % fit.n_features_)\n",
    "    fs = X.columns[fit.support_].tolist()   # selected features\n",
    "    print(\"Selected Features: %s\" % fs)\n",
    "    scores = cross_val_score(model, X[fs], y, cv=10)\n",
    "    print(\"Acc: \"+str(scores.mean()))\n",
    "\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.feature_selection import RFE\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate,cross_val_score\n",
    "\n",
    "#forward selection\n",
    "cat_model = CatBoostClassifier(depth=6,random_state=random_state_num)\n",
    "\n",
    "sfs1 = SFS(cat_model, k_features=len(X.columns),          # number of features\n",
    "            verbose=2,scoring='accuracy',cv=2)\n",
    "sfs1 = sfs1.fit(X, y)\n",
    "print(sfs1.subsets_)             # selection process\n",
    "print(sfs1.k_feature_idx_)       # selected feature index\n",
    "print(sfs1.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Example: Add classifiers from external libraries\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "model1 = RandomForestClassifier(random_state=random_state_num)\n",
    "model2 = XGBClassifier(random_state=random_state_num)\n",
    "model3 = GradientBoostingClassifier(random_state=random_state_num)\n",
    "model4 = LogisticRegression(random_state=random_state_num)\n",
    "model5 = LGBMClassifier(random_state=random_state_num)\n",
    "model6 = SVC(kernel='linear', random_state=random_state_num)\n",
    "model7 = DecisionTreeClassifier(random_state=random_state_num)\n",
    "\n",
    "# Example: Add more classifiers\n",
    "model8 = CatBoostClassifier()\n",
    "model9 = GaussianNB()\n",
    "model10 = MLPClassifier(random_state=random_state_num)\n",
    "model11 = AdaBoostClassifier(random_state=random_state_num)\n",
    "model12 = BaggingClassifier(random_state=random_state_num)\n",
    "model13 = ExtraTreesClassifier(random_state=random_state_num)\n",
    "model14 = VotingClassifier(estimators=[('cat', model9), ('xgb', model2), ('svc', model6),('adaboost',model11),('bagging',model12),('lgbm',model5)], voting='hard')\n",
    "model15 = StackingClassifier(estimators=[('svc', model6), ('xgb', model2),('adaboost',model11),('bagging',model12),('lgbm',model5)], final_estimator=model9)\n",
    "model16 = HistGradientBoostingClassifier(random_state=random_state_num)\n",
    "\n",
    "models = [model1, model2, model3, model4, model5, model6, model7, model8, model9, model10,\n",
    "          model11, model12, model13, model14, model15, model16]\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    result = accuracy_score(y_test, pred)\n",
    "    results.append(result)\n",
    "\n",
    "for i in range(0, len(results)):\n",
    "    print(f\"Model {i  + 1}: Accuracy = {results[i]:.4f}\")\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space for each model\n",
    "    if model_type == 'RandomForest':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state_num)\n",
    "    elif model_type == 'XGBoost':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 1.0)\n",
    "        model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                              random_state=random_state_num)\n",
    "    elif model_type == 'GradientBoosting':\n",
    "\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 1.0)\n",
    "        model = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                           learning_rate=learning_rate, random_state=random_state_num)\n",
    "    elif model_type == 'LogisticRegression':\n",
    "        C = trial.suggest_loguniform('C', 0.001, 1000.0)\n",
    "        model = LogisticRegression(C=C, random_state=random_state_num)\n",
    "    elif model_type == 'LGBMClassifier':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 1.0)\n",
    "        model = LGBMClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                               random_state=random_state_num)\n",
    "    elif model_type == 'SVC':\n",
    "        C = trial.suggest_loguniform('C', 0.001, 1000.0)\n",
    "        model = SVC(C=C, kernel='linear', random_state=random_state_num)\n",
    "    elif model_type == 'DecisionTree':\n",
    "        max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "        model = DecisionTreeClassifier(max_depth=max_depth, random_state=random_state_num)\n",
    "    elif model_type == 'CatBoost':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 1.0)\n",
    "        model = CatBoostClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                   random_state=random_state_num)\n",
    "    elif model_type == 'GaussianNB':\n",
    "        model = GaussianNB()\n",
    "    elif model_type == 'MLPClassifier':\n",
    "        model = MLPClassifier(random_state=random_state_num)\n",
    "    elif model_type == 'AdaBoostClassifier':\n",
    "        model = AdaBoostClassifier(random_state=random_state_num)\n",
    "    elif model_type == 'BaggingClassifier':\n",
    "        model = BaggingClassifier(random_state=random_state_num)\n",
    "    elif model_type == 'ExtraTreesClassifier':\n",
    "        model = ExtraTreesClassifier(random_state=random_state_num)\n",
    "    elif model_type == 'VotingClassifier':\n",
    "        model = VotingClassifier(estimators=[('cat', model9), ('xgb', model2), ('svc', model6),('adaboost',model11),('bagging',model12),('lgbm',model5)], voting='hard')\n",
    "    elif model_type == 'StackingClassifier':\n",
    "        model = StackingClassifier(estimators=[('svc', model6), ('xgb', model2),('adaboost',model11),('bagging',model12),('lgbm',model5)], final_estimator=model9)\n",
    "\n",
    "\n",
    "\n",
    "    # Add other models and their hyperparameters here\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "for index, model in enumerate(models):\n",
    "    if index == 0:\n",
    "        model_type = 'RandomForest'\n",
    "    elif index == 1:\n",
    "        model_type = 'XGBoost'\n",
    "    elif index == 2:\n",
    "        model_type = 'GradientBoosting'\n",
    "    elif index == 3:\n",
    "        model_type = 'LogisticRegression'\n",
    "    elif index == 4:\n",
    "        model_type = 'LGBMClassifier'\n",
    "    elif index == 5:\n",
    "        model_type = 'SVC'\n",
    "    elif index == 6:\n",
    "        model_type = 'DecisionTree'\n",
    "    elif index == 7:\n",
    "        model_type = 'CatBoost'\n",
    "    elif index == 8:\n",
    "        model_type = 'GaussianNB'\n",
    "    elif index == 9:\n",
    "        model_type = 'MLPClassifier'\n",
    "    elif index == 10:\n",
    "        model_type = 'AdaBoost'\n",
    "    elif index == 11:\n",
    "        model_type = 'Bagging'\n",
    "    elif index == 12:\n",
    "        model_type = 'ExtraTrees'\n",
    "    elif index == 13:\n",
    "        model_type = 'Voting'\n",
    "    elif index == 14:\n",
    "        model_type = 'Stacking'\n",
    "    elif index == 15:\n",
    "        model_type = 'HistGradientBoosting'\n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_accuracy = study.best_value\n",
    "\n",
    "    print(f\"Model {index + 1} ({model_type}): Best Accuracy = {best_accuracy:.4f}\")\n",
    "    print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    best_model = model.set_params(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    pred = best_model.predict(X_test)\n",
    "    result = accuracy_score(y_test, pred)\n",
    "    results.append(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
